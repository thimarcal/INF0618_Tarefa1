{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 01 - Exploring CNNs for MNIST\n",
    "\n",
    "In this assignment, we want you to modify the CNN architecture that we used in the last MNIST exercise, adding new layers and altering their hyperparameters. We have already loaded and preprocessed the data for you, so you can focus on the architecture and training of your network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------\n",
    "*** You do not need to alter the blocks `Imports`, `Load training data` and `Preprocessing` ***\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from random import shuffle, seed\n",
    "seed(42)  #keep this seed, in order to compare the results with your classmates\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (15,15) # Make the figures a bit bigger\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = 10\n",
    "\n",
    "# the data, shuffled and split between trainVal and test sets\n",
    "(trainVal_data, trainVal_label), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# We want now to split the trainVal data into train and validation sets\n",
    "nData = trainVal_data.shape[0]  #find the size of trainVal\n",
    "nTrain = int(nData * 0.8)  #80% to train, 20% to val\n",
    "\n",
    "randomIdx = list(range(nData))   #randomly select indexes\n",
    "shuffle(randomIdx)\n",
    "trainIdx = randomIdx[:nTrain] \n",
    "valIdx = randomIdx[nTrain:]\n",
    "\n",
    "# Split the data\n",
    "X_val, y_val = trainVal_data[valIdx], trainVal_label[valIdx]\n",
    "X_train, y_train = trainVal_data[trainIdx], trainVal_label[trainIdx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain class weights and samples per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train --->  {0: 4724, 1: 5393, 2: 4723, 3: 4881, 4: 4704, 5: 4313, 6: 4769, 7: 5001, 8: 4730, 9: 4762} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print(\"Train ---> \", dict(zip(unique, counts)), \"\\n\")\n",
    "\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weights = compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "train_class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "unique, counts = np.unique(y_test, return_counts=True)\n",
    "test_sample_per_class = counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "#The first dimension refers to the number of images\n",
    "X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "X_val = X_val.reshape(X_val.shape[0], img_rows, img_cols, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_val = X_val.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "X_train /= 255\n",
    "X_val /= 255\n",
    "X_test /= 255\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_val = np_utils.to_categorical(y_val, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------\n",
    "------------------------------------------\n",
    "------------------------------------------\n",
    "------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Your assignment starts here!!!***\n",
    "\n",
    "# Task 1 [0.25 pts] - Add a fully-connected layer\n",
    "Let's investigate if the network gets better as we add more layers to it. We want you to add another fully-connected layer to the network from last exercise. Your network will have:\n",
    "- Convolutional layer with 10 filters of size 5x5 with ReLU activation;\n",
    "- Max pooling layer with kernel 2x2, that will reduce each spatial dimension by half (24x24 to 12x12);\n",
    "- An operation to flatten the feature maps into an array of size 10x12x12 = 1440\n",
    "- Dropout operation with probability 0.25, applied to flattened array\n",
    "- **Fully connected layer with 100 units/neurons and ReLU activation;**\n",
    "- Fully connected layer with units/neurons equal to the number of classes in our problem (in this case, 10);\n",
    "- Softmax activation on the last FC layer.\n",
    "\n",
    "The architecture will look like this:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"task1_cnn.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define your model here\n",
    "model = Sequential()\n",
    "\n",
    "# Add the Convolution Layer with 5x5 Kernel and 10 filters\n",
    "model.add(Conv2D(10, \n",
    "                 kernel_size=(5,5),\n",
    "                 activation='relu',\n",
    "                 input_shape=(28,28,1)))\n",
    "\n",
    "# Add pooling layer with kernel 2x2\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flatten\n",
    "model.add(Flatten())\n",
    "\n",
    "# Dropout with probability 0.25\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Fully Connected layer with 100 units and ReLU activation\n",
    "model.add(Dense(100, activation='relu'))\n",
    "\n",
    "# FC layer with 10 units and Softmax\n",
    "model.add(Dense(nb_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/4\n",
      "48000/48000 [==============================] - 24s 507us/step - loss: 1.0838 - acc: 0.6832 - val_loss: 0.4488 - val_acc: 0.8712\n",
      "Epoch 2/4\n",
      "48000/48000 [==============================] - 23s 489us/step - loss: 0.4617 - acc: 0.8636 - val_loss: 0.3478 - val_acc: 0.8978\n",
      "Epoch 3/4\n",
      "48000/48000 [==============================] - 23s 489us/step - loss: 0.3882 - acc: 0.8860 - val_loss: 0.3045 - val_acc: 0.9105 0\n",
      "Epoch 4/4\n",
      "48000/48000 [==============================] - 23s 474us/step - loss: 0.3471 - acc: 0.8975 - val_loss: 0.2762 - val_acc: 0.9200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22557376710>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile and train your model\n",
    "sgd = optimizers.SGD(lr=0.01) #lr = learning rate\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, Y_train,\n",
    "          batch_size=128, epochs=4, verbose=1,\n",
    "          class_weight = train_class_weights,\n",
    "          validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 221us/step\n",
      "Test loss: 0.2525385090440512\n",
      "Test accuracy (NOT NORMALIZED): 0.9281\n",
      "{0: 0.9795918367346939, 1: 0.9806167400881057, 2: 0.8798449612403101, 3: 0.9207920792079208, 4: 0.9358452138492872, 5: 0.905829596412556, 6: 0.9467640918580376, 7: 0.9134241245136187, 8: 0.9014373716632443, 9: 0.910802775024777} \n",
      "\n",
      "Normalized Acc -->  0.9274948790592552\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, Y_test, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy (NOT NORMALIZED):', score[1])\n",
    "\n",
    "\n",
    "predicted_classes = model.predict_classes(X_test)\n",
    "Y_test_classes = np.argmax(Y_test, axis=-1)\n",
    "\n",
    "accPerClass = []\n",
    "for classIdx in range(nb_classes):\n",
    "    idx = (Y_test_classes == classIdx)\n",
    "    \n",
    "    correctPred = np.sum(predicted_classes[idx] == Y_test_classes[idx])\n",
    "    accPerClass.append( correctPred / float(test_sample_per_class[classIdx]))\n",
    "    \n",
    "print(dict(zip(range(nb_classes),accPerClass)), \"\\n\")\n",
    "print(\"Normalized Acc --> \", np.mean(accPerClass))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "----------------\n",
    "# Task 2 [0.25 pts] - Add another (conv + max pooling) layers\n",
    "We want you to add another convolutional layer, followed by a max pooling to the network. Your network will have:\n",
    "- Convolutional layer with 10 filters of size 5x5 with ReLU activation;\n",
    "- Max pooling layer with kernel 2x2, that will reduce each spatial dimension by half (24x24 to 12x12);\n",
    "- **Convolutional layer with 20 filters of size 5x5 with ReLU activation;**\n",
    "- **Max pooling layer with kernel 2x2, that will reduce each spatial dimension by half (8x8 to 4x4);**\n",
    "- An operation to flatten the feature maps into an array of size 20x4x4 = 320\n",
    "- Dropout operation with probability 0.25, applied to flattened array\n",
    "- Fully connected layer with units/neurons equal to the number of classes in our problem (in this case, 10);\n",
    "- Softmax activation on the last FC layer.\n",
    "\n",
    "The architecture will look like this:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"task2_cnn.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define your model here\n",
    "model = Sequential()\n",
    "\n",
    "# Add the Convolution Layer with 5x5 Kernel and 10 filters\n",
    "model.add(Conv2D(10, \n",
    "                 kernel_size=(5,5),\n",
    "                 activation='relu',\n",
    "                 input_shape=(28,28,1)))\n",
    "\n",
    "# Add pooling layer with kernel 2x2\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Add another Convolutional Layer 20 filters and 5x5 kernel\n",
    "model.add(Conv2D(20, \n",
    "                 kernel_size=(5,5),\n",
    "                 activation='relu'))\n",
    "\n",
    "# Add pooling layer with kernel 2x2\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flatten\n",
    "model.add(Flatten())\n",
    "\n",
    "# Dropout with probability 0.25\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# FC layer with 10 units and Softmax\n",
    "model.add(Dense(nb_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/4\n",
      "48000/48000 [==============================] - 27s 572us/step - loss: 1.7830 - acc: 0.4231 - val_loss: 0.7087 - val_acc: 0.8189\n",
      "Epoch 2/4\n",
      "48000/48000 [==============================] - 30s 628us/step - loss: 0.5873 - acc: 0.8171 - val_loss: 0.3424 - val_acc: 0.9081\n",
      "Epoch 3/4\n",
      "48000/48000 [==============================] - 30s 630us/step - loss: 0.3782 - acc: 0.8850 - val_loss: 0.2527 - val_acc: 0.9297\n",
      "Epoch 4/4\n",
      "48000/48000 [==============================] - 31s 639us/step - loss: 0.2967 - acc: 0.9105 - val_loss: 0.2108 - val_acc: 0.9392\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22554435320>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile and train your model\n",
    "sgd = optimizers.SGD(lr=0.01) #lr = learning rate\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, Y_train,\n",
    "          batch_size=128, epochs=4, verbose=1,\n",
    "          class_weight = train_class_weights,\n",
    "          validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 3s 305us/step\n",
      "Test loss: 0.1902854533225298\n",
      "Test accuracy (NOT NORMALIZED): 0.9469\n",
      "{0: 0.9816326530612245, 1: 0.9806167400881057, 2: 0.9321705426356589, 3: 0.9386138613861386, 4: 0.9429735234215886, 5: 0.952914798206278, 6: 0.9603340292275574, 7: 0.914396887159533, 8: 0.9271047227926078, 9: 0.9365708622398414} \n",
      "\n",
      "Normalized Acc -->  0.9467328620218535\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, Y_test, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy (NOT NORMALIZED):', score[1])\n",
    "\n",
    "\n",
    "predicted_classes = model.predict_classes(X_test)\n",
    "Y_test_classes = np.argmax(Y_test, axis=-1)\n",
    "\n",
    "accPerClass = []\n",
    "for classIdx in range(nb_classes):\n",
    "    idx = (Y_test_classes == classIdx)\n",
    "    \n",
    "    correctPred = np.sum(predicted_classes[idx] == Y_test_classes[idx])\n",
    "    accPerClass.append( correctPred / float(test_sample_per_class[classIdx]))\n",
    "    \n",
    "print(dict(zip(range(nb_classes),accPerClass)), \"\\n\")\n",
    "print(\"Normalized Acc --> \", np.mean(accPerClass))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "----------------\n",
    "# Task 3 [0.25 pts] - Add Conv + MaxPool + FC\n",
    "Now combine tasks 1 and 2; adding the convolutional, max pooling and fc layer to your network. Your CNN will have:\n",
    "- Convolutional layer with 10 filters of size 5x5 with ReLU activation;\n",
    "- Max pooling layer with kernel 2x2, that will reduce each spatial dimension by half (24x24 to 12x12);\n",
    "- **Convolutional layer with 20 filters of size 5x5 with ReLU activation;**\n",
    "- **Max pooling layer with kernel 2x2, that will reduce each spatial dimension by half (8x8 to 4x4);**\n",
    "- An operation to flatten the feature maps into an array of size 20x4x4 = 320\n",
    "- Dropout operation with probability 0.25, applied to flattened array\n",
    "- **Fully connected layer with 100 units/neurons and ReLU activation;**\n",
    "- Fully connected layer with units/neurons equal to the number of classes in our problem (in this case, 10);\n",
    "- Softmax activation on the last FC layer.\n",
    "\n",
    "The architecture will look like this:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"task3_cnn.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define your model here\n",
    "model = Sequential()\n",
    "\n",
    "# Add the Convolution Layer with 5x5 Kernel and 10 filters\n",
    "model.add(Conv2D(10, \n",
    "                 kernel_size=(5,5),\n",
    "                 activation='relu',\n",
    "                 input_shape=(28,28,1)))\n",
    "\n",
    "# Add pooling layer with kernel 2x2\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Add another Convolutional Layer 20 filters and 5x5 kernel\n",
    "model.add(Conv2D(20, \n",
    "                 kernel_size=(5,5),\n",
    "                 activation='relu'))\n",
    "\n",
    "# Add pooling layer with kernel 2x2\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flatten\n",
    "model.add(Flatten())\n",
    "\n",
    "# Dropout with probability 0.25\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Fully Connected layer with 100 units and ReLU activation\n",
    "model.add(Dense(100, activation='relu'))\n",
    "\n",
    "# FC layer with 10 units and Softmax\n",
    "model.add(Dense(nb_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/4\n",
      "48000/48000 [==============================] - 32s 666us/step - loss: 1.6217 - acc: 0.4920 - val_loss: 0.5057 - val_acc: 0.8684\n",
      "Epoch 2/4\n",
      "48000/48000 [==============================] - 31s 653us/step - loss: 0.4853 - acc: 0.8529 - val_loss: 0.2820 - val_acc: 0.9206\n",
      "Epoch 3/4\n",
      "48000/48000 [==============================] - 31s 653us/step - loss: 0.3289 - acc: 0.9004 - val_loss: 0.2206 - val_acc: 0.9389\n",
      "Epoch 4/4\n",
      "48000/48000 [==============================] - 31s 651us/step - loss: 0.2665 - acc: 0.9195 - val_loss: 0.1855 - val_acc: 0.9469\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22559c5a6d8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile and train your model\n",
    "sgd = optimizers.SGD(lr=0.01) #lr = learning rate\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, Y_train,\n",
    "          batch_size=128, epochs=4, verbose=1,\n",
    "          class_weight = train_class_weights,\n",
    "          validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 3s 317us/step\n",
      "Test loss: 0.16640454709231853\n",
      "Test accuracy (NOT NORMALIZED): 0.9501\n",
      "{0: 0.9877551020408163, 1: 0.9850220264317181, 2: 0.9418604651162791, 3: 0.9455445544554455, 4: 0.929735234215886, 5: 0.9316143497757847, 6: 0.9624217118997912, 7: 0.9289883268482491, 8: 0.9373716632443532, 9: 0.9454905847373637} \n",
      "\n",
      "Normalized Acc -->  0.9495804018765686\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, Y_test, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy (NOT NORMALIZED):', score[1])\n",
    "\n",
    "\n",
    "predicted_classes = model.predict_classes(X_test)\n",
    "Y_test_classes = np.argmax(Y_test, axis=-1)\n",
    "\n",
    "accPerClass = []\n",
    "for classIdx in range(nb_classes):\n",
    "    idx = (Y_test_classes == classIdx)\n",
    "    \n",
    "    correctPred = np.sum(predicted_classes[idx] == Y_test_classes[idx])\n",
    "    accPerClass.append( correctPred / float(test_sample_per_class[classIdx]))\n",
    "    \n",
    "print(dict(zip(range(nb_classes),accPerClass)), \"\\n\")\n",
    "print(\"Normalized Acc --> \", np.mean(accPerClass))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "----------------\n",
    "# Task 4 [0.25 pts] - Implement a new modification\n",
    "Implement **one** modification to your network and evaluate it. Some possible alterations are:\n",
    "- Add more convolutional and/or max pooling layers;\n",
    "- Alter the kernel size and number of filters of the conv layers;\n",
    "- Try training with different batch sizes and higher number of epochs;\n",
    "- Try with different activations, besides ReLU and Softmax;\n",
    "- Try optimizing the CNN with a different loss;\n",
    "- Try different learning rates;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define your model here\n",
    "model = Sequential()\n",
    "\n",
    "# Add the Convolution Layer with 5x5 Kernel and 10 filters\n",
    "model.add(Conv2D(10, \n",
    "                 kernel_size=(5,5),\n",
    "                 activation='relu',\n",
    "                 input_shape=(28,28,1)))\n",
    "\n",
    "# Add pooling layer with kernel 2x2\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Add another Convolutional Layer 30 filters and 5x5 kernel\n",
    "model.add(Conv2D(30, \n",
    "                 kernel_size=(5,5),\n",
    "                 activation='relu'))\n",
    "\n",
    "# Add another Convolutional Layer 20 filters and 3x3 kernel\n",
    "model.add(Conv2D(20, \n",
    "                 kernel_size=(3,3),\n",
    "                 activation='relu'))\n",
    "\n",
    "# Add pooling layer with kernel 2x2\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "# Flatten\n",
    "model.add(Flatten())\n",
    "\n",
    "# Dropout with probability 0.25\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Fully Connected layer with 50 units and ReLU activation\n",
    "model.add(Dense(50, activation='relu'))\n",
    "\n",
    "# FC layer with 10 units and Softmax\n",
    "model.add(Dense(nb_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/4\n",
      "48000/48000 [==============================] - 38s 787us/step - loss: 0.6290 - acc: 0.7938 - val_loss: 0.1364 - val_acc: 0.9583\n",
      "Epoch 2/4\n",
      "48000/48000 [==============================] - 38s 783us/step - loss: 0.1581 - acc: 0.9512 - val_loss: 0.0909 - val_acc: 0.9718: 6s - loss: 0.1645 - acc: 0.9 - ETA: 6\n",
      "Epoch 3/4\n",
      "48000/48000 [==============================] - 37s 777us/step - loss: 0.1155 - acc: 0.9638 - val_loss: 0.0710 - val_acc: 0.9782\n",
      "Epoch 4/4\n",
      "48000/48000 [==============================] - 37s 778us/step - loss: 0.0972 - acc: 0.9698 - val_loss: 0.0635 - val_acc: 0.9801\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2255a092588>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile and train your model\n",
    "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, Y_train,\n",
    "          batch_size=128, epochs=4, verbose=1,\n",
    "          class_weight = train_class_weights,\n",
    "          validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on the Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 4s 382us/step\n",
      "Test loss: 0.05020933179943822\n",
      "Test accuracy (NOT NORMALIZED): 0.985\n",
      "{0: 0.9948979591836735, 1: 0.9920704845814978, 2: 0.9786821705426356, 3: 0.9841584158415841, 4: 0.9826883910386965, 5: 0.9876681614349776, 6: 0.988517745302714, 7: 0.9854085603112841, 8: 0.9856262833675564, 9: 0.9702675916749257} \n",
      "\n",
      "Normalized Acc -->  0.9849985763279546\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, Y_test, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy (NOT NORMALIZED):', score[1])\n",
    "\n",
    "\n",
    "predicted_classes = model.predict_classes(X_test)\n",
    "Y_test_classes = np.argmax(Y_test, axis=-1)\n",
    "\n",
    "accPerClass = []\n",
    "for classIdx in range(nb_classes):\n",
    "    idx = (Y_test_classes == classIdx)\n",
    "    \n",
    "    correctPred = np.sum(predicted_classes[idx] == Y_test_classes[idx])\n",
    "    accPerClass.append( correctPred / float(test_sample_per_class[classIdx]))\n",
    "    \n",
    "print(dict(zip(range(nb_classes),accPerClass)), \"\\n\")\n",
    "print(\"Normalized Acc --> \", np.mean(accPerClass))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
